{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# RadNet AI for Optimising Radiotherapy Outcomes Workshop - Coding demonstration\n",
    "\n",
    "This notebook will show, briefly, how to build an autosegmentation model for thoracic OARs using pytorch and pytorch-lightning. We will be using some open data from [The Cancer Imaging Archive (TCIA)](https://www.cancerimagingarchive.net/), originally used for a [AAPM challenge](http://www.autocontouringchallenge.org/). This dataset contains 60 patients, each of which has five OARs segmented.\n",
    "\n",
    "To handle the data, we will use [pydicom](https://github.com/pydicom/pydicom) to load slices and ideas from [dicom-contour](https://github.com/KeremTurgutlu/dicom-contour) to convert RTSTRUCT objects into masks.\n",
    "\n",
    "We will be using a suite of pre-built pytorch segmentation models in the excellent [segmentation-models](https://github.com/qubvel/segmentation_models.pytorch) package. This package simplifies the building of a pretrained segmentation network in 2D. We will use a 2D approach, looping over slices in the data to segment 3D organs.\n",
    "\n",
    "Pytorch can be quite intimidating, but is very powerful when you get to grips with it. In the interests of simplicity, we will use a wrapper around pytorch called [pytorch-lightning](https://pytorch-lightning.readthedocs.io/en/latest/). Lightning separates out the different bits of ML, allowing you to write a bit less boilerplate code, and letting us very quickly and easily use best-practise methods to train our models.\n",
    "\n",
    "\n",
    "# Overview\n",
    "The steps in this notebook make the following steps:\n",
    "\n",
    "0. Install prerequisites and set up\n",
    "1. Load DICOM data containing CT and segmentation and convert to numpy arrays\n",
    "2. Define some preprocessing and apply it to the CT slices\n",
    "3. Create a segmentation model, using a library to make a pre-trained model for our segmentation task\n",
    "4. Train a the model to reproduce the training examples\n",
    "5. Test the model against the testing data and produce the AAPM competition ranking score"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# How to use colab & jupyter notebooks\n",
    "If you're new to colab and/or jupyter notebooks, here are some tips on how they work.\n",
    "\n",
    "## Colab\n",
    "Colab is a free ML playground from google. It allows you free access to limited resources, including a GPU and some storage space. Even though the limits are quite small: ~30GB disk, 12GB RAM & a random GPU from K80 up to P100, you can do some pretty cool stuff with it. You will need a Google account to sign into it.\n",
    "\n",
    "For us, we are going to be training a Convolutional Neural Network (CNN), so we need to get a GPU. To do this, click \"Runtime\" in the menu across the top of the colab page, then select \"Change Runtime Type\". From the dropdown, select GPU and click save. The runtime will then reboot and you should have a GPU. To find out what you got, run the cell below this text.\n",
    "\n",
    "## Jupyter\n",
    "Jupyter is a tool for running python in a notebook form. A notebook is simply a document containing code and accompanying text describing/explaning the code. You're reading one right now!\n",
    "\n",
    "Notebooks are divided into cells which, for the most part, come in two flavours - Markdown and code. A markdown cell is where you can type words to explain the code. Try double clicking on this text, and you should be shown the markdown that created it. \n",
    "\n",
    "Code cells contain python code. There are a couple of things to bear in mind about notebooks that differ from normal python scripts:\n",
    "- Notebook cells can be run in any order\n",
    "- The output of any cell is available in any other cell (as if it were all global scope in python)\n",
    "- Typos can really screw you over. If you make a typo in a variable name, that variable still exists, and anywhere where you made the same typo will use the old variable instead of the new one. This has personally led to at least three hours debugging that could have been saved by being able to spell.\n",
    "- Default plotting behaviour is to just give you a picture with no interactivity. We can override it though\n",
    "\n",
    "All cells are executed by either clicking the play button at the top left corner of the cell, or by clicking in it and pressing ctrl+enter. You can also press shift+enter, which will run the curent cell and move to the next.\n",
    "\n",
    "### Jupyter escapes and magic\n",
    "You will see a few cells with lines starting in either an exclamation mark (!) or a percentage sign (%). These lines are called escapes and magics. An escape simply makes jupyter run the command after it in a bash shell (or cmd if you're on windows), this allows us to do stuff like run wget to download data.\n",
    "\n",
    "Magics do things to alter the state of jupyter, for example by turning matplotlib interactivity on, or enabling the browser-in-browser that allows us to use tensorboard monitoring. The most important thing to note about magics is that they can't have a comment after them. You can look up some jupyter magics [here](https://ipython.readthedocs.io/en/stable/interactive/magics.html).\n",
    "\n",
    "\n",
    "Now you know how to drive this notebook, let's start running some stuff!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find out what GPU we got (and make sure we actually have one!)\n",
    "!nvidia-smi"
   ]
  },
  {
   "source": [
    "Because of the volume of data and the time we have, I've saved the output of each step (where applicable). This is because colab has a RAM limit of 12GB and doing some of this requires more than that and will crash the runtime. The code for each step is in the notebook, but we will be doing \"Blue Peter\" here's one I prepared earlier at pretty much every step.\n",
    "\n",
    "The cell below downloads all the necessary data and places it on the colab machine. Run it, and then you can see how the data was manipulated in the functions, but don't have to run it if we don't have time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download all the data! This may take a little while...\n",
    "\n",
    "## raw DICOM\n",
    "!wget -nc https://www.dropbox.com/s/o0t9kw30b3m08fi/LCTSC.tar.gz?dl=0 -O /content/LCTSC.tar.gz\n",
    "!mkdir /content/LCTSC\n",
    "!tar -xf /content/LCTSC.tar.gz -C /content/LCTSC\n",
    "!ls /content/LCTSC\n",
    "!rm LCTSC.tar.gz\n",
    "\n",
    "## Processed data\n",
    "!wget https://www.dropbox.com/s/v66iwbh4wbibv2a/AllProcessedData.tar?dl=0 -O /content/AllProcessedData.tar\n",
    "!tar -xf /content/AllProcessedData.tar -C /content/\n",
    "!rm AllProcessedData.tar\n",
    "\n",
    "## Network weights\n",
    "!wget https://www.dropbox.com/s/nl6kavhfm3x2qtu/pretrained_checkpoint.ckpt?dl=0 -O /content/pretrained_checkpoint.ckpt"
   ]
  },
  {
   "source": [
    "## 0. Install prerequisites\n",
    "\n",
    "Here we install the pytorch flavour of the segmentation-models library, along with pydicom and pytorch-lightning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==1.8.1+cu102 torchvision==0.9.1+cu102 torchaudio===0.8.1 torchtext==0.9 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
    "%pip install git+https://github.com/qubvel/segmentation_models.pytorch\n",
    "%pip install pydicom pytorch_lightning tqdm ipympl"
   ]
  },
  {
   "source": [
    "## 0. Set up monitoring and enable matplotlib notebook interactivity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "source": [
    "## 0. Load required libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import directed_hausdorff\n",
    "import segmentation_models_pytorch as smp\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.spatial import KDTree\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm.notebook import tqdm\n",
    "import pytorch_lightning as pl\n",
    "import scipy.ndimage as scn\n",
    "import albumentations as A\n",
    "from matplotlib import cm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "## These are the structures defined in the TCIA data\n",
    "structure_names = ['SpinalCord', 'Lung_R', 'Lung_L', 'Heart', 'Esophagus']\n",
    "## See if we're in colab...\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "  datapath = \"/content/\"\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  datapath = \"E:/code/RadNetPractical/\"  ## <---- You will need to change this if running locally"
   ]
  },
  {
   "source": [
    "At this point, we may start to skip running things during the live demo. This is purely because of the time constraints we have. I will still go through each cell, and we will look at the output of each cell when I load the Blue Peter packs.\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load and visualise the data\n",
    "Earlier, we downloaded and decompressed a load of data for us to use. Part of that is the raw DICOM from the TCIA, the cells below will process that data into numpy arrays, which is what we need to do our ML with.\n",
    "\n",
    "The data is provided in DICOM format, as most RT data will be when we want to use it for ML. In the following cell, I define some functions that will help us load the DICOM data and convert it into numpy arrays so that we can use it to train an ML model. Each function is described in its own docstring, but the broad idea is to get the UID for every slice in a CT image, then look for any structure that references that UID. Then we transform the coordinates of the contour points from the DICOM frame of reference into image pixels, and burn those pixels into a mask. Then we can use a binary hole filling to create a solid 2D mask. This process is repeated for every slice that has a contour on, for every ROI we want to create a full 3D mask. Note that I set the pixel value based on the index of the ROI - this will become the pixel's class label later on.\n",
    "\n",
    "We have to do this now because of the way segmentation models work. 'Classic' CNNs classify an image into one of several classes (e.g. dog, bird, cat etc). This classification is across the whole image, and if there were to be an image containing both a dog and cat, it would be difficult to classify that image, since the network can \"see\" both things. The next level of complexity is object detection networks; these work by drawing a bounding box around each object in the image - this means they can handle images with more than one class in. However, when we're doing radiotherapy, or lots of other tasks, we need to know exactly which bits of the image are what object - this is where segmentation, or semantic segmentation comes in. Semantic segmentation gives a class label to every pixel in the image, allowing it to accurately track the edges of organs and other things.\n",
    "\n",
    "To train a semantic segmentation model, we need labels for every pixel in the image. These are derived from the DICOM RTSTRUCT file, but we have convert contours (a series of points in space) to masks (images with 0 for background, 1 for foreground). Lots of the code in the cell below is inspired by [dicom-contour](https://github.com/KeremTurgutlu/dicom-contour), but doesn't make the assumption that filename = SOPInstanceUID. This cell loops over all the patients and their available contours and links up the contour with the correct image slice, converting it to a mask on the way. This is what we then use to train our network.\n",
    "\n",
    "The cell below also contains a handy little function for compositing a mask over a CT slice. You can do it with matplotlib, but this way is ~50x faster, which matters when you're looking at really big datasets. We will optionally use it to make a folder of sanity check images in a bit."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Andrew Green\n",
    "Description: This function takes a slice of CT and a segmentation mask and produces an overlaid\n",
    "image like what plt.imshow would so, but about 50x faster\n",
    "\n",
    "Depends on PIL, numpy and matplotlib. To install PIL, do `pip install pillow`\n",
    "\"\"\"\n",
    "from PIL import Image\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "def fastOverlay(imageSlice, classes, outputPath, window=350, level=50, alpha=0.5, num_class=3):\n",
    "    \"\"\"\n",
    "    Fast overlay image code. Used in my bulk segmenter to produce sanity check images\n",
    "\n",
    "    This code makes the assumption that the slice and the mask are the exact same shape, so make\n",
    "    sure that's the case.\n",
    "\n",
    "    Still uses matplotlib to get the colourmaps, so you can't completely get rid of it yet.\n",
    "\n",
    "    Arguments:\n",
    "        imageSlice (Required): The CT slice for your background. The default level and window assume \n",
    "                                it has paid the worldmatch tax (i.e +1024 from HU)\n",
    "        classes (Required): The array of pixel classes. This is, e.g. the output of your CNN after \n",
    "                            you've done argmax on it. Tested with float, but should work with int.\n",
    "        outputPath (Required): The full path to where you want the uimage to be put. \n",
    "\n",
    "        window (Optional): The window to use for visualisation. Assumes Worldmatch numbers. Default 1600.\n",
    "        level (Optional): The level to use for visualisation. Assumes Worldmatch numbers. Default 1600.\n",
    "        alpha (Optional): The transparency for the structure mask you're overlaying. Should be [0,1]. Default 0.5\n",
    "        num_class (Optional): The maximum class index used in constructing the class colourmap. Default 3\n",
    "\n",
    "    \"\"\"\n",
    "    cmap = cm.get_cmap('Greys_r')\n",
    "    norm = Normalize(vmin=level-window//2, vmax=level+window//2) ## use normalise to apply level and window. \n",
    "    ## NB Normalize is equivalent to setting vmin & vmax in plt.imshow\n",
    "\n",
    "    cmap_mask = cm.get_cmap('viridis')\n",
    "    norm_mask = Normalize(vmin=1.0, vmax=num_class) \n",
    "    ## these two lines convert the mask into a colour image that PIL will be able to understand\n",
    "\n",
    "    ## Appky W/L and convert slice to PIL Image\n",
    "    wld_slice = cmap(norm(imageSlice))\n",
    "    slice_image = Image.fromarray((wld_slice[:, :, :3] * 255).astype(np.uint8)) \n",
    "\n",
    "    ## apply class colourmap and convert to PIL Image\n",
    "    transf_mask = cmap_mask(norm_mask(classes))\n",
    "    mask_image = Image.fromarray((transf_mask[:, :, :3] * 255).astype(np.uint8))\n",
    "\n",
    "    ## Generate mask for compositing - should be informed by alpha (i.e. for alpha=0.5, should be 128 in areas where the masks are)\n",
    "    compo_mask = np.ones(imageSlice.shape, dtype=np.uint8) * np.uint8(255*alpha) ## default alpha 0.5\n",
    "    compo_mask[classes == 0] = 255 ## transparent background\n",
    "    ## This is 255 not zero because it is allowing all of the background through. It's a bit backwards.\n",
    "    compo_mask_image = Image.fromarray(compo_mask) \n",
    "    ## Now create the overlaid image\n",
    "    compost = Image.composite(slice_image, mask_image, compo_mask_image)\n",
    "    ## Save to specified path\n",
    "    compost.save(outputPath)\n",
    "\n",
    "\n",
    "def load_image(slices_fpath):\n",
    "    \"\"\"\n",
    "    This functions load the DICOM corresponding to an image, but doesn't actually load the pixels. \n",
    "    Instead, we just keep copies of the whole DICOM object for each slice, which then includes other stuff.\n",
    "    Crucially, we sort the returned list on the patient's position so that the slices and other returned \n",
    "    things are in the correct order\n",
    "    \"\"\"\n",
    "    slices = []\n",
    "    for slice_fname in  os.listdir(slices_fpath):\n",
    "        try:\n",
    "            slice_f = pydicom.dcmread(os.path.join(slices_fpath, slice_fname))\n",
    "            slice_f.pixel_array ## in case there's an RTSTRUCT\n",
    "            assert slice_f.Modality != \"RTDOSE\"\n",
    "            slices.append(slice_f)\n",
    "        except:\n",
    "            continue\n",
    "    slices = sorted(slices, key=lambda s: s.ImagePositionPatient[-1])\n",
    "    uids = [s.SOPInstanceUID for s in slices]\n",
    "    pixels = np.array([(float(s.PixelSpacing[0]), float(s.PixelSpacing[1]))  for s in slices])\n",
    "    origins = np.array([s.ImagePositionPatient for s in slices])\n",
    "\n",
    "    return slices, uids, pixels, origins\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_image_and_mask(contour_filepath, slices_filepath, filter_slices=True):\n",
    "    \"\"\"\n",
    "    This function loads the CT image and the RTSTRUCT to go with it. It uses the load_image function to organise the CT, and then \n",
    "    loops on structures, then contours of the structures. In each iteration, we find the correct slice number from the matching UID\n",
    "    and grab the details about that slice from the loaded list. DICOM coordinates are mapped into pixels, and those pixels are burned\n",
    "    into a mask using a sparse matrix representation. We then fill holes in the mask to get a proper filled mask to use with training.\n",
    "\n",
    "    Lots of this is inspired by dicom_contour, but adjusted for generalisability, e.g. not making assumptions about filenames\n",
    "    \"\"\"\n",
    "    ## load and sort the image first\n",
    "    slices, uids, pixels, origins = load_image(slices_filepath)\n",
    "\n",
    "    ## Create arrays - load pixel data in at this point\n",
    "    img_array = np.zeros((len(slices), *slices[0].pixel_array.shape) , dtype=np.int16)\n",
    "    \n",
    "    ## Very important to apply the rescale & intercept - pydicom doesn't do it by default\n",
    "    for idx, img_slice in enumerate(slices):\n",
    "        img_array[idx, ...] = pydicom.pixel_data_handlers.apply_rescale(img_slice.pixel_array, img_slice)\n",
    "\n",
    "    ## Create an array for the masks, same shape as the image\n",
    "    mask_array = np.zeros_like(img_array, dtype=np.int8)\n",
    "\n",
    "    ## Load RTSTRUCT and loop over contours in it\n",
    "    rtst_file = pydicom.dcmread(contour_filepath)\n",
    "    roi_seq_names = [roi_seq.ROIName for roi_seq in list(rtst_file.StructureSetROISequence)]\n",
    "    for name in roi_seq_names: ## loop on structures: SC, LR, LL, H, E ?\n",
    "        structure_id = structure_names.index(name) + 1 ## structure ID from our list - make sure they're all the same!\n",
    "        rtv = rtst_file.ROIContourSequence[roi_seq_names.index(name)]\n",
    "        contours = [contour for contour in rtv.ContourSequence] ## Loop on slices with contours\n",
    "        for c in contours:\n",
    "            refd_UID = c.ContourImageSequence[0].ReferencedSOPInstanceUID\n",
    "            loc_index = uids.index(refd_UID)\n",
    "            x_pixel, y_pixel = pixels[loc_index]\n",
    "            x_origin, y_origin, _ = origins[loc_index]\n",
    "            contour_coord = c.ContourData\n",
    "\n",
    "            # x, y, z coordinates of the contour in mm\n",
    "            x0 = contour_coord[len(contour_coord)-3]\n",
    "            y0 = contour_coord[len(contour_coord)-2]\n",
    "            z0 = contour_coord[len(contour_coord)-1]\n",
    "            coord = []\n",
    "            for i in range(0, len(contour_coord), 3):\n",
    "                x = contour_coord[i]\n",
    "                y = contour_coord[i+1]\n",
    "                z = contour_coord[i+2]\n",
    "                l = np.sqrt((x-x0)*(x-x0) + (y-y0)*(y-y0) + (z-z0)*(z-z0))\n",
    "                l = np.ceil(l*2)+1\n",
    "                for j in np.arange(1, l+1):\n",
    "                    coord.append([(x-x0)*j/l+x0, (y-y0)*j/l+y0, (z-z0)*j/l+z0])\n",
    "                x0 = x\n",
    "                y0 = y\n",
    "                z0 = z\n",
    "\n",
    "            # y, x is how it's mapped\n",
    "            pixel_coords = [(np.round((y - y_origin) / y_pixel), np.round((x - x_origin) / x_pixel)) for x, y, _ in coord]\n",
    "\n",
    "            rows = []\n",
    "            cols = []\n",
    "            for i,j in list(set(pixel_coords)):\n",
    "                rows.append(i)\n",
    "                cols.append(j)\n",
    "            \n",
    "            ## Burn edge pixels into array\n",
    "            contour_arr = csc_matrix((np.ones_like(rows), (rows, cols)), dtype=np.int8, shape=(img_array.shape[1], img_array.shape[2])).toarray()\n",
    "            ## Fill burned contour to make mask\n",
    "            filled_arr = scn.binary_fill_holes(contour_arr)\n",
    "            ## Now multiply mask (all 0s & 1s) with structure ID (range 1 - 5) to correctly label the pixel\n",
    "            ## Note the += - without that, structures split into more than one part (e.g. lungs) will overwrite each other!\n",
    "            mask_array[loc_index,...] += filled_arr  * structure_id\n",
    "        mask_array[mask_array > 5] = 0\n",
    "\n",
    "    if filter_slices:\n",
    "        ## filter out slices with no segmentation on them\n",
    "        delineated_slices = np.sum(mask_array, axis=(1,2)) > 0\n",
    "        return img_array[delineated_slices,...], mask_array[delineated_slices,...], origins[delineated_slices,...], pixels[delineated_slices]\n",
    "    else:\n",
    "        return img_array, mask_array, origins, pixels"
   ]
  },
  {
   "source": [
    "# STOP! \n",
    "## Read what is below before you decide whether to run this cell!\n",
    "Now that we have the functions we need, we can go ahead and load up the DICOM data. Be warned, this cell will take a loooong time to run (only about 15 minutes). It is looping over all the patients we have in the LCTSC folder, and adding them into a big pair of arrays with matched CT slice and segmentation mask. These arrays will eventually use a few GB of RAM, and potentially quite a bit more while being built. \n",
    "\n",
    "__Depending on time, we can skip this cell and load the result from the downloaded data in a cell below.__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptid_structs = \"Not\"\n",
    "ptid_img = \"Initialised\"\n",
    "\n",
    "\n",
    "train_annotated_ct_slices = None\n",
    "train_mask_slices = None\n",
    "train_slice_locations = None\n",
    "train_pixel_sizes = None\n",
    "train_pt_lengths = []\n",
    "\n",
    "test_annotated_ct_slices = None\n",
    "test_mask_slices = None\n",
    "test_slice_locations = None\n",
    "test_pixel_sizes = None\n",
    "test_pt_lengths = []\n",
    "\n",
    "with tqdm(total=60, desc=\"Patients processed\") as pbar:\n",
    "    for root, dirs, files in os.walk(os.path.join(datapath, \"LCTSC\")):\n",
    "\n",
    "        #folders containing dicom files have no subdirectories\n",
    "        if len(dirs) == 0:\n",
    "            if len(files) == 1:\n",
    "                # This is the RTSTRUCT folder\n",
    "                rt_struct_file = os.path.join(root, files[0])\n",
    "                ptid_structs = pydicom.dcmread(rt_struct_file).PatientName\n",
    "            else:\n",
    "                # otherwise, this is an image folder\n",
    "                img_data_path = root\n",
    "                img_file = pydicom.read_file(os.path.join(root, files[0]))\n",
    "                ptid_img = img_file.PatientName\n",
    "\n",
    "            if ptid_structs == ptid_img:\n",
    "                ### Only filter unsegmented slices in the training data - leave test data alone\n",
    "                ct_image, mask_image, origins, pixels = get_image_and_mask(rt_struct_file, img_data_path, filter_slices=re.search(\"-Train-\", root))\n",
    "\n",
    "                if re.search(\"-Train-\", root):\n",
    "                    train_pt_lengths.append(ct_image.shape[0])\n",
    "                    if train_annotated_ct_slices is None:\n",
    "                        train_annotated_ct_slices = ct_image.copy()\n",
    "                        train_mask_slices = mask_image.copy()\n",
    "                        train_slice_locations = origins.copy()\n",
    "                        train_pixel_sizes = pixels.copy()\n",
    "                    else:\n",
    "                        train_annotated_ct_slices = np.vstack((train_annotated_ct_slices, ct_image))\n",
    "                        train_mask_slices = np.vstack((train_mask_slices, mask_image))\n",
    "                        train_slice_locations = np.vstack((train_slice_locations, origins))\n",
    "                        train_pixel_sizes = np.vstack((train_pixel_sizes, pixels))\n",
    "                else:\n",
    "                    test_pt_lengths.append(ct_image.shape[0])\n",
    "                    if test_annotated_ct_slices is None:\n",
    "                        test_annotated_ct_slices = ct_image.copy()\n",
    "                        test_mask_slices = mask_image.copy()\n",
    "                        test_slice_locations = origins.copy()\n",
    "                        test_pixel_sizes = pixels.copy()\n",
    "                    else:\n",
    "                        test_annotated_ct_slices = np.vstack((test_annotated_ct_slices, ct_image))\n",
    "                        test_mask_slices = np.vstack((test_mask_slices, mask_image))\n",
    "                        test_slice_locations = np.vstack((test_slice_locations, origins))\n",
    "                        test_pixel_sizes = np.vstack((test_pixel_sizes, pixels))\n",
    "                    \n",
    "\n",
    "                pbar.update(1)\n",
    "                \n",
    "write_sanity = False\n",
    "if write_sanity:\n",
    "    for i in range(test_annotated_ct_slices.shape[0]):\n",
    "        fastOverlay(test_annotated_ct_slices[i,...], test_mask_slices[i,...], f\"sanity_loading/{i:04d}.png\")"
   ]
  },
  {
   "source": [
    "If you want to be faster and just load the preprocessed data, run the next cell"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(os.path.join(datapath, \"train_annotated_ct_slices.npz\"))\n",
    "train_annotated_ct_slices = train_data[\"train_annotated_ct_slices\"]\n",
    "train_slice_locations = train_data[\"train_slice_locations\"]\n",
    "train_pixel_sizes = train_data[\"train_pixel_sizes\"]\n",
    "train_pt_lengths = train_data[\"train_pt_lengths\"]\n",
    "\n",
    "\n",
    "test_data = np.load(os.path.join(datapath, \"test_annotated_ct_slices.npz\"))\n",
    "test_annotated_ct_slices = test_data[\"test_annotated_ct_slices\"]\n",
    "test_slice_locations = test_data[\"test_slice_locations\"]\n",
    "test_pixel_sizes = test_data[\"test_pixel_sizes\"]\n",
    "test_pt_lengths = test_data[\"test_pt_lengths\"]\n",
    "\n",
    "\n",
    "print(train_annotated_ct_slices.shape)\n",
    "print(test_annotated_ct_slices.shape)\n",
    "\n",
    "train_mask_slices = np.load(os.path.join(datapath, \"train_mask_slices.npz\"))[\"train_mask_slices\"]\n",
    "test_mask_slices = np.load(os.path.join(datapath, \"test_mask_slices.npz\"))[\"test_mask_slices\"]\n",
    "\n",
    "print(train_mask_slices.shape)\n",
    "print(test_mask_slices.shape)\n",
    "\n",
    "del train_data\n",
    "del test_data"
   ]
  },
  {
   "source": [
    "# 2. Preprocessing\n",
    "\n",
    "Preprocessing your data is an extremely important part of machine learning, and so I'm going to do a little bit here. preprocessing is used to standardise the images and, especially in machine learning, to compress their intensities down to a given range (usually 0-1). \n",
    "\n",
    "The preprocessing I will write in the next cell is the simplest I can come up with that will still do the job. We will standardise the images by applying a level/window transformation and standardising the output in the range 0-255. The slices are then used as if they were normal grayscale images, and the preprocessing/augmentation pipeline takes care of the rest. \n",
    "\n",
    "First we define a window/level like function and use it to set a mediastinal window on the data - most of what we're trying to visualise is in that region, so this should be ok."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_level(data, window=350, level=50):\n",
    "    \"\"\"\n",
    "    Apply a window and level transformation to CT slices. \n",
    "\n",
    "    The default values are taken taken from https://radiopaedia.org/articles/windowing-ct?lang=gb and are recommended for visualising the mediastinum\n",
    "    \n",
    "    The returned array will be NxHxWx3, as we expand the array into 3 channels. Values will be in the range 0-255 and type will be uint8 to mimic a 'normal' image\n",
    "    \"\"\"\n",
    "    ## calculate high & low edges of level & window\n",
    "    low_edge  = level - (window//2)\n",
    "    high_edge = level + (window//2)\n",
    "    ## use np.clip to clip into that level/window, then adjust to range 0 - 255 and convert to uint8\n",
    "    windowed_data = (((np.clip(data, low_edge, high_edge) - low_edge)/window) * 255).astype(np.uint8)\n",
    "    \n",
    "\n",
    "    ## repeat the array in the last axis to make a 3 channel image\n",
    "    # windowed_data = np.repeat(windowed_data, 3, axis=-1)\n",
    "\n",
    "    return windowed_data"
   ]
  },
  {
   "source": [
    "Now we can apply this transformation to the data we loaded from the DICOM. This doesn't take too long, but there is a pre-processed npz file if you're impatient."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the preprocessing \n",
    "window_levelled_slices_train = window_level(train_annotated_ct_slices)\n",
    "window_levelled_slices_test = window_level(test_annotated_ct_slices)\n",
    "\n",
    "## If you run out of RAM, you can save memory by deleting the original array - will need to re-load it if we change something\n",
    "# del train_annotated_ct_slices\n",
    "# del test_annotated_ct_slices\n",
    "\n",
    "print(window_levelled_slices_test.shape)\n",
    "\n"
   ]
  },
  {
   "source": [
    "And the cell for the impatient:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_levelled_slices_train = np.load(os.path.join(datapath, \"window_levelled_slices_train.npz\"))[\"window_levelled_slices_train\"]\n",
    "window_levelled_slices_test  = np.load(os.path.join(datapath, \"window_levelled_slices_test.npz\"))[\"window_levelled_slices_test\"]"
   ]
  },
  {
   "source": [
    "## Sanity checking\n",
    "It is a good idea to periodically check that your data actually makes sense. I call these sanity checks, and they are as simple as just plotting the CT slice with the masks overlaid and making sure they look more or less lined up. Let's do this quickly now"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(window_levelled_slices_train[7,...], cmap='Greys_r')\n",
    "plt.imshow(train_mask_slices[7,...].squeeze(), alpha=0.5)"
   ]
  },
  {
   "source": [
    "If you've been paying attention, you will know that we have ~5000 images to train on. That's a lot. To make training a bit more tractable, I will now randomly select ~1500 training examples and ~500 validation ones. Ideally, you would just use the whole dataset, but we will either run out of memory or time if we do. We set the numpy random seed to a known value so everyone should get roughly the same results, then re-seed randomly afterwards so nothing else is affected."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "subset_indices = np.random.randint(0, window_levelled_slices_test.shape[0], size=1500)\n",
    "\n",
    "wl_slice_subset_train = window_levelled_slices_train[subset_indices[0:1000]]\n",
    "mask_subset_train = train_mask_slices[subset_indices[0:1000]]\n",
    "\n",
    "wl_slice_subset_val = window_levelled_slices_train[subset_indices[-500:]]\n",
    "mask_subset_val =  train_mask_slices[subset_indices[-500:]]\n",
    "\n",
    "del window_levelled_slices_train\n",
    "del train_mask_slices\n",
    "\n",
    "np.random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Another sanity check to be sure that did what we expected...\n",
    "plt.figure()\n",
    "plt.imshow(wl_slice_subset_train[7,...], cmap='Greys_r')\n",
    "plt.imshow(mask_subset_train[7,...].squeeze(), alpha=0.5)\n",
    "print(wl_slice_subset_train.shape)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Now we can build the data loading, augmentation and normalisation pipeline. This is a little more involved in pytorch than in keras for example, but still isn't too difficult.\n",
    "\n",
    "In the cell below, we create a subclass of the pytorch Dataset object, specific to our task. This class is made from three arguments - the array of images, the array of masks and a set of transformations we wish to apply. The two arrays are easy, we already have them ready to go, but the transformations may take a little bit of thinking.\n",
    "\n",
    "These transformations are where we can introduce some data augmentation. Recall that data augmentation is the process of applying random transformations to our data to create \"new\" synthetic data to train our model with. We said at the beginning we would only use a little bit of data augmentation, namely horizontal flipping and a little bit of rotation. We will set that up in the next cell.\n",
    "\n",
    "There are also some transformations that are more or less mandatory. For example, pytorch's pretrained models expect images to be of type float, and have a specific mean and standard deviation (derived from the imagenet dataset). We apply a transformation that handles normalisaing to the imagenet mean, and converting the array into a pytorch tensor so it can be sent through the model. Because we're working on a single channel, we normalise to the mean of means, and mean standard deviation across the RGB channels (This will hopefully make more sense in the code.)\n",
    "\n",
    "We will use a library called albumentations to handle our augmentations. It is very fast and easy to use, but only works for 2D images. For 3D augmentations, there is a library called kornia which can do some augmentations.\n",
    "\n",
    "It is important to note that we use different pipelines for the training and validation data. For training, we can do whatever we like to make the data go as far as possible, but when validating, we're meant to be getting an idea of the network's performance on 'real' data, so we should not do any augmentation. This is easy to do with the way things are set up."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a subclass of Dataset that handles our image-mask pair loaded from arrays\n",
    "## This is the bare minimum example!\n",
    "class LCTSCDataGen(torch.utils.data.Dataset):\n",
    "  def __init__(self, image_array, mask_array, transform):\n",
    "    super().__init__()\n",
    "    self.image_array = image_array\n",
    "    self.mask_array = mask_array\n",
    "    self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.image_array.shape[0]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    image = self.image_array[idx,...]\n",
    "    mask = self.mask_array[idx, ...]\n",
    "    if self.transform is not None:\n",
    "      transformed = self.transform(image=image, mask=mask)\n",
    "      image = transformed['image']\n",
    "      mask = transformed['mask']\n",
    "    # print(mask.shape)\n",
    "    return image[np.newaxis,...], mask\n",
    "\n",
    "## Now create the augmentation pipeline\n",
    "\n",
    "train_transforms = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(5),\n",
    "    A.Normalize(mean=(np.mean([0.485, 0.456, 0.406])), std=(np.mean([0.229, 0.224, 0.225]))) ## Note mean of means, mean of stds\n",
    "])\n",
    "\n",
    "## valdation pipeline just does normalisation and conversion to tensor\n",
    "val_transforms = A.Compose([\n",
    "    A.Normalize(mean=(np.mean([0.485, 0.456, 0.406])), std=(np.mean([0.229, 0.224, 0.225]))) ## Note mean of means, mean of stds\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "## Now create some datasets and dataloaders\n",
    "train_dataset = LCTSCDataGen(wl_slice_subset_train, mask_subset_train, train_transforms)\n",
    "val_dataset = LCTSCDataGen(wl_slice_subset_val, mask_subset_val, val_transforms)\n",
    "\n",
    "## Create dataloaders from these datasets\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "print(mask_subset_train.shape, wl_slice_subset_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One more sanity check...\n",
    "train_img, train_msk = next(iter(train_dataloader))\n",
    "\n",
    "# print(train_img.numpy()[3,...].shape)\n",
    "# print(train_msk.numpy()[3,...].shape)\n",
    "# print(train_msk.numpy()[3,250, 200], train_msk.numpy()[3,250,350])\n",
    "plt.figure()\n",
    "plt.imshow(train_img.numpy()[3,...].squeeze(), cmap='Greys_r')\n",
    "plt.imshow(train_msk.numpy()[3,...].squeeze(), alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "source": [
    "# 3. Creating the Segmentation model\n",
    "\n",
    "For simplicity, we are going to use a python library of pre-built segmentation networks. The library is called `segmentation_models` and I highly reccomend you have a look at the [github page](https://github.com/qubvel/segmentation_models.pytorch). We already imported the library at the top of the notebook, so we can use it straight away here.\n",
    "\n",
    "We will be using a pre-trained segmentation model, in which the feature extraction parts of the network have been pre-trained on imagenet (A large database of natural images). This should mean we can train a good model with relatively little data, but also means we have to convert our 1 channel CT image into a 3 channel RGB image. Fortunately, the library can do the 1 -> 3 channel conversion for us by repeating the image in the channel dimension.\n",
    "\n",
    "We will be using an architecture called a Feature Pyramid Network or [FPN](), it looks something like this:\n",
    "\n",
    "![FPN architecture, from segmentation_models github page](https://github.com/qubvel/segmentation_models/raw/master/images/fpn.png)\n",
    "\n",
    "The part on the left, in gray, is the bit that is pretrained on imagenet; this is called the backbone of the network. Pretraining means that the backbone already knows about some features in images that are useful, in this case to classify them. We will use these features as a starting point from which we will learn features useful to our task - segmentation in CT.\n",
    "\n",
    "The choice of backbone is somewhat arbitrary - more modern and bigger networks should have better performance, but not always. We will use the smallest network available, ResNet-18 because it will hopefully train faster and use less GPU memory. \n",
    "\n",
    "As I alluded to at the beginning, writing/explaining a pytorch training loop in the ~1 hour we have is probably not realistic, so we are going to use a library called pytorch-lightning to do all the heavy lifting for us. To be able to use it, we have to wrap our pytorch model in a special class that inherits from a LightningModule (If none of that makes sense, don't worry - this bit is making the model so that pytorch-lightning knows what to do with it). To do the wrapping, we create a class LightningFPN, and define a few mandatory functions for pytorch-lightning to use.\n",
    "\n",
    "While creating the wrapper, we also have to select which optimiser to use, and the most appropriate loss function for the problem. There are lots of considerations in selecting an optimiser, but most people use one of either Stochastic Gradient Descent (SGD) with momentum, or Adaptive Moment Estimation (Adam). Usually, Adam converges faster than SGD because of the fancy stuff it is doing inside the optimiser, but sometimes SGD can find a better solution by avoiding a local minimum. In the interests of speed, we will use Adam.\n",
    "\n",
    "The choice of loss function can also have a profound impact on the quality of the trained model. There are a few loss functions we could consider here. Segmentation is just a classification problem applied to every pixel in the image, so we could use a loss designed for classification, apply it in every pixel and then take the average across the whole image. This is what the categorical crossentropy loss will do. Categorical crossentropy can fall over though - especially when there is a class imbalance. Since we are segmenting organs that occupy a small fraction of the image, we have a lot of background and not much foreground, therefore the loss will be dominated by the background. There are ways around this (e.g. weighted cross entropy, or focal loss) but as a first attempt, we should probably use something else.\n",
    "\n",
    "The Dice similarity coefficient (DSC) is very well known, and can be used as a loss function here. DSC is rightly villified in radiotherapy because it has no spatial component (how far apart were the contours) and is very sensitive to volume (small contours will always be much worse than big ones). The segmentation_models library has implementations of all these losses; for now we will just use DSC, but it would be trivial to use one of the other losses.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the class that will wrap te pytorch model up for ptl\n",
    "class LightningFPN(pl.LightningModule):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    ## Create the pytorch model \n",
    "    self.model = smp.FPN(\"resnet18\", in_channels=1, classes=len(structure_names)+1, encoder_weights='imagenet')\n",
    "    \n",
    "    ## Construct a loss function, this is DSC, configured for multiple classes, and ignoring the background\n",
    "    self.loss_fcn = smp.losses.DiceLoss(\"multiclass\", from_logits=True, ignore_index=0)\n",
    "\n",
    "    ## Specify which optimiser to use here\n",
    "    self.optimizer = torch.optim.Adam\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.model(x)\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = self.optimizer(self.parameters(), lr=1e-4)## May need to handle other kwargs here!\n",
    "    return {\"optimizer\": optimizer, \"reduce_on_plateau\":True}\n",
    "    ## Note - we are reducing the learning rate when the validation loss plateaus for a while - this should improve the model\n",
    "\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    img, msk = batch\n",
    "    msk_hat = self(img)\n",
    "    loss = self.loss_fcn(msk_hat, msk.long())\n",
    "    self.log(\"loss\", loss)\n",
    "    return loss\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    img, msk = batch\n",
    "    msk_hat = self(img)\n",
    "    val_loss = self.loss_fcn(msk_hat, msk.long())\n",
    "    self.log(\"val_loss\", val_loss)\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "## Now we can wrap the prebuilt model up inside a pytorch lightning module:\n",
    "\n",
    "pl_model = LightningFPN()\n",
    "\n",
    "## Done!\n"
   ]
  },
  {
   "source": [
    "This model has about 13 million parameters, which might stretch our GPU a bit - we will need to think about batch size if this becomes an issue during training.\n",
    "\n",
    "# 4. Training\n",
    "\n",
    "We're now ready to train the model. It is very easy when using pytorch-lightning, only taking 2 lines to do what would be hundreds in pure pytorch!\n",
    "\n",
    "To be able to keep track of what is going on, we will use the tensorboard log viewer, which I activate in the next cell. This will allow us to see the training and validation loss change as the network learns.\n",
    "\n",
    "To keep things quick, we will only train for 5 epochs. Ideally, we would train for a few hundred, to make sure the network loss is properly saturated.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(gpus=1, max_epochs=5)\n",
    "trainer.fit(pl_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "source": [
    "You should be able to see the training progress in the tensorboard browser. 5 epochs is not enough, but as we will see, it is actually surprisingly good...\n",
    "\n",
    "We can now try running this model on some test data. I've chosen a slice somewhat at random so that it has all the structures, in a moment we will run the segmentation over the whole test set.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = test_pt_lengths[0]//3\n",
    "## Note - we have to manually expand the channel dimension\n",
    "in_image = val_transforms(image=window_levelled_slices_test[idx:idx+1,:,:][:,np.newaxis,...])['image'] \n",
    "\n",
    "gt = test_mask_slices[idx:idx+1,...].astype(np.float32)\n",
    "gt[gt == 0.0] = np.nan ## make the background invisible\n",
    "\n",
    "test = pl_model.model.predict(torch.tensor(in_image)).detach() ## the model dosn't do softmax activation, so we have to do it ourselves\n",
    "probs = torch.nn.functional.softmax(test).numpy().squeeze()\n",
    "test_mask = np.argmax(probs, axis=0).astype(np.float)\n",
    "test_mask[test_mask == 0.0] = np.nan\n",
    "\n",
    "\n",
    "## Show the results\n",
    "fig =  plt.figure(figsize=(10,10)) \n",
    "ax_gt = fig.add_subplot(121)\n",
    "ax_gt.set_title(\"Ground Truth\")\n",
    "ax_cnn = fig.add_subplot(122)\n",
    "ax_cnn.set_title(\"CNN contour\")\n",
    "\n",
    "ax_gt.imshow(in_image.squeeze(), cmap='Greys_r')\n",
    "ax_gt.imshow(gt.squeeze(), alpha=0.75, cmap='Pastel2', vmin=1, vmax=5)\n",
    "\n",
    "ax_cnn.imshow(in_image.squeeze(), cmap='Greys_r')\n",
    "ax_cnn.imshow(test_mask, alpha=0.75, cmap='Pastel2', vmin=1, vmax=5)"
   ]
  },
  {
   "source": [
    "Again, to keep this from taking all day, I have pre-trained a model on the exact same data as here, but for 200 epochs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained = pl_model.load_from_checkpoint(os.path.join(datapath, \"pretrained_checkpoint.ckpt\"))\n",
    "\n",
    "in_image = val_transforms(image=window_levelled_slices_test[idx:idx+1,:,:][:,np.newaxis,...])['image'] \n",
    "\n",
    "gt = test_mask_slices[idx,...].astype(np.float32)\n",
    "gt[gt == 0.0] = np.nan\n",
    "\n",
    "test = model_trained.model.predict(torch.tensor(in_image)).detach() ## the model dosn't do softmax activation, so we have to do it ourselves\n",
    "probs = torch.nn.functional.softmax(test).numpy().squeeze()\n",
    "test_mask = np.argmax(probs, axis=0).astype(np.float)\n",
    "test_mask[test_mask == 0.0] = np.nan\n",
    "\n",
    "## Show the results\n",
    "fig =  plt.figure(figsize=(10,10)) \n",
    "ax_gt = fig.add_subplot(121)\n",
    "ax_gt.set_title(\"Ground Truth\")\n",
    "ax_cnn = fig.add_subplot(122)\n",
    "ax_cnn.set_title(\"CNN contour\")\n",
    "\n",
    "ax_gt.imshow(in_image.squeeze(), cmap='Greys_r')\n",
    "ax_gt.imshow(gt.squeeze(), alpha=0.75, cmap='Pastel2', vmin=1, vmax=5)\n",
    "\n",
    "ax_cnn.imshow(in_image.squeeze(), cmap='Greys_r')\n",
    "ax_cnn.imshow(test_mask, alpha=0.75, cmap='Pastel2', vmin=1, vmax=5)\n"
   ]
  },
  {
   "source": [
    "# 5. Evaluation\n",
    "\n",
    "Visually, the results look pretty good, but now let's try to quantify it a bit. We're going to use a few metrics: \n",
    "- Dice Coefficient - the same things as what we used for the loss, just measures overlap and is sensitive to \n",
    "- Mean surface distance - The mean of the directed hausdorf distance from GT -> CNN and CNN -> GT\n",
    "- 95th percentile Hausdorff distance - the mean of the directed 95th percentile HD from GT -> CNN and CNN -> GT\n",
    "\n",
    "These are the same metrics as used in the original AAPM challenge. Unfortunately, I don't have access to the reference values used in the calculation of the score, otherwise we could see where we rank!\n",
    "\n",
    "The cell below defines some functions to help us segment in 3D and calculate the metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_3d(image, model, transforms):\n",
    "    batch_size = 8\n",
    "    whole_batches = image.shape[0] // batch_size\n",
    "    batch_splitpoints = [(a*batch_size, a*batch_size + batch_size) for a in  range(whole_batches)]\n",
    "\n",
    "    ## do things on GPU\n",
    "    model.cuda()\n",
    "\n",
    "    if image.shape[0] % batch_size != 0:\n",
    "        last_batch_start_idx = whole_batches * batch_size\n",
    "        last_batch_size = image.shape[0] - last_batch_start_idx\n",
    "        batch_splitpoints.append((last_batch_start_idx, image.shape[0]))\n",
    "    \n",
    "    segmentation = np.zeros_like(image)\n",
    "    for b_start, b_stop in batch_splitpoints:\n",
    "        transformed_image = torch.tensor(transforms(image=image[b_start:b_stop])['image']).cuda()\n",
    "        logits = model.predict(transformed_image[:, np.newaxis,...]).cpu()\n",
    "        probs = torch.nn.functional.softmax(logits)\n",
    "        segmentation[b_start:b_stop,...] = np.argmax(probs, axis=1)\n",
    "    return segmentation\n",
    "\n",
    "def mask_2_contour(mask, origin, pixel_size):\n",
    "    \"\"\"\n",
    "    Gives back corrdinates on the contours in mm, in the same FoR as the original DICOM\n",
    "\n",
    "    This is only a little step from being able to write an RTSTRUCT file...\n",
    "    \"\"\"\n",
    "    slice_structures = {}\n",
    "    for structure_idx in range(len(structure_names)):\n",
    "        structure_name = structure_names[structure_idx]\n",
    "        struc = np.zeros_like(mask, dtype=np.uint8)\n",
    "        struc[mask == structure_idx+1] = 255  ## Structure ID is index +1\n",
    "        contours, hierachy = cv2.findContours(struc, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        slice_structures[structure_name] = []\n",
    "\n",
    "        for c in contours:\n",
    "            if cv2.contourArea(c) < 50: ## ignore tiny contours - should remove little blobs segmented by the network\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            if len(c) > 1:\n",
    "                contour_y = c.squeeze()[:,0]\n",
    "                contour_x = c.squeeze()[:,1]\n",
    "                x_pixel, y_pixel = pixel_size\n",
    "                x_origin, y_origin, z_origin = origin\n",
    "\n",
    "\n",
    "                # y, x is how it's mapped\n",
    "                dicom_coords = [( (y*y_pixel) + y_origin, (x * x_pixel) + x_origin, z_origin ) for x, y in zip(contour_y, contour_x) ]\n",
    "                slice_structures[structure_name].append(dicom_coords)\n",
    "    return slice_structures\n",
    "\n",
    "def mask_2_contour_3d(mask, origins, pixel_sizes):\n",
    "    \"\"\"\n",
    "    Calculate contours on each slice independently, then build into list for 3D\n",
    "    \"\"\"\n",
    "    all_structure_contours = {sn:[] for sn in structure_names}\n",
    "    for mask_slice, origin, pixel in zip(mask, origins, pixel_sizes):\n",
    "        this_slice_structures = mask_2_contour(mask_slice, origin, pixel)\n",
    "\n",
    "        for structure_name in structure_names:\n",
    "            all_structure_contours[structure_name].extend(this_slice_structures[structure_name])\n",
    "    return all_structure_contours\n",
    "\n",
    "\n",
    "def dice_coefficient(gt, cnn):\n",
    "    dice_summary = {}\n",
    "    for structure_name in structure_names:\n",
    "        structure_idx = structure_names.index(structure_name) + 1\n",
    "        dice = np.sum(np.logical_and(gt==structure_idx, cnn==structure_idx))*2.0 / (np.sum(cnn==structure_idx) + np.sum(gt==structure_idx) )\n",
    "        dice_summary[structure_name] = dice\n",
    "    return dice_summary\n",
    "\n",
    "def mean_surface_distance(gt, cnn):\n",
    "    \"\"\"\n",
    "    Calculate undirected HD both ways then return average\n",
    "    \"\"\"\n",
    "    r1, i1, i2 = directed_hausdorff(gt, cnn)\n",
    "    r2, i1, i2 = directed_hausdorff(gt, cnn)\n",
    "\n",
    "    return (r1 + r2)/2 \n",
    "\n",
    "def hausdorff_distances(points_gt, points_cnn):\n",
    "    \"\"\"\n",
    "    Inspiration from https://github.com/hjkuijf/wmhchallenge/blob/master/evaluation.py\n",
    "\n",
    "    Gives the same values as scipy directed_hausdorff when percentile is changed to max, so I *think* it is correct\n",
    "    \"\"\"\n",
    "    def getDistancesFromAtoB(a, b):    \n",
    "        kdTree = KDTree(a, leafsize=100)\n",
    "        return kdTree.query(b, k=1, eps=0, p=2)[0]\n",
    "\n",
    "    # Compute distances from test to result; and result to test\n",
    "    dTestToResult = getDistancesFromAtoB(points_gt, points_cnn)\n",
    "    dResultToTest = getDistancesFromAtoB(points_cnn, points_gt)    \n",
    "    \n",
    "    return max(np.percentile(dTestToResult, 95) , np.percentile(dResultToTest, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first segment the image in 3D by running on every slice\n",
    "idx = test_pt_lengths[0]\n",
    "test_segmentation = segment_3d(window_levelled_slices_test[0:idx,:,:], model_trained.model, val_transforms)\n",
    "gt = test_mask_slices[0:idx,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gt_contours = mask_2_contour_3d(gt, test_slice_locations[0:idx], test_pixel_sizes[0:idx])\n",
    "\n",
    "## Only compare where GT has contours\n",
    "comparison_indices = np.sum(gt, axis=(1,2)) > 0\n",
    "\n",
    "cnn_contours = mask_2_contour_3d(test_segmentation[comparison_indices,...], test_slice_locations[0:idx][comparison_indices], test_pixel_sizes[0:idx][comparison_indices])\n",
    "\n",
    "\n",
    "## calculate DSC\n",
    "dice_test = dice_coefficient(gt[comparison_indices,...], test_segmentation[comparison_indices,...])\n",
    "print(dice_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cnn = {name : [] for name in structure_names}\n",
    "all_gt = {name : [] for name in structure_names}\n",
    "\n",
    "distance_measures = {name : {} for name in structure_names}\n",
    "\n",
    "for name in structure_names:\n",
    "    [all_cnn[name].extend(list(a)) for a in cnn_contours[name]]\n",
    "    all_cnn[name] = np.array(all_cnn[name])\n",
    "\n",
    "    [all_gt[name].extend(list(a)) for a in gt_contours[name]]\n",
    "    all_gt[name] = np.array(all_gt[name])\n",
    "\n",
    "    distance_measures[name]['msd'] = mean_surface_distance(all_gt[name], all_cnn[name])\n",
    "    distance_measures[name][\"95HD\"] = hausdorff_distances(all_gt[name], all_cnn[name])\n",
    "\n",
    "    print(f\"{name}\\n\\tMSD: {distance_measures[name]['msd']:.3f}\\n\\t95HD: {distance_measures[name]['95HD']:.3f}\\n\\tDSC: {dice_test[name]:.3f}\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ]
}